data:
  source:
    split_path: '/fast_scratch_2/fcormier/t2k/ml/data/oct20_combine_flatE/train_val_test_nFolds3_fold0.npz' 
    dataset:
      h5file: '/fast_scratch_2/fcormier/t2k/ml/data/oct20_combine_flatE/combine_combine.hy'
      _target_: watchmal.dataset.cnn.cnn_dataset.CNNDataset
      pmt_positions_file: '/fast_scratch/WatChMaL/data/T2K/image_files/skdetsim_imagefile.npy'
      channel_scaling:
        time: [400, 1000]
  target:
    split_path: '/fast_scratch_2/fcormier/t2k/ml/data/oct20_combine_flatE/train_val_test_nFolds3_fold0.npz' 
    dataset:
      h5file: '/fast_scratch_2/fcormier/t2k/ml/data/oct20_combine_flatE/combine_combine.hy'
      _target_: watchmal.dataset.cnn.cnn_dataset.CNNDatasetDeadPMT
      pmt_positions_file: '/fast_scratch/WatChMaL/data/T2K/image_files/skdetsim_imagefile.npy'
      channel_scaling:
        time: [400, 1000]
      dead_pmt_rate: 0.05
      dead_pmt_seed: 42  

model:
  _target_: watchmal.model.dann.DANNModel
  label_predictor:
    _target_: watchmal.model.dann.LabelPredictor
    feature_extractor:
      _target_: watchmal.model.resnet.resnet34
      num_input_channels: 2
      num_output_channels: 1
      stride: 1
      kernelSize: 1
      dropout_p: 0.
    label_pred:
      _target_: watchmal.model.dann.FlexibleNNClassifier
      input_dim: 512   
      hidden_dims: [128]  
      output_dim: 1
      dropout_p: 0.01
  domain_classifier:
    _target_: watchmal.model.dann.FlexibleNNClassifier
    input_dim: 512
    hidden_dims: [128]
    output_dim: 1
    dropout_p: 0.1

engine:
  _target_: watchmal.engine.regression_dann.DANNRegressionEngine
  truth_key: 'momenta'
  pretrained_model_path: /data/thoriba/t2k/training/resnet_jul15_muon_mom_without_mask/13072024-040810_copy1/RegressionEngine_ResNet_BEST.pth
  domain_pre_train_epochs: 0
  domain_in_train_itrs: 2
  max_lammy: 1.0

tasks:
  train:
    epochs: 50
    val_interval: 25
    num_val_batches: 2
    checkpointing: false
    loss:
      classification:
        _target_: torch.nn.HuberLoss
        delta: 0.1
      domain:
        _target_: torch.nn.BCEWithLogitsLoss
    data_loaders:
      source_train:
        split_key: train_idxs
        batch_size: 16
        num_workers: 1
        sampler:
          # use sequential for debugging
          _target_: torch.utils.data.sampler.SubsetRandomSampler
      target_train:
        split_key: train_idxs
        batch_size: 16
        num_workers: 1
        sampler:
          # use sequential for debugging
          _target_: torch.utils.data.sampler.SubsetRandomSampler
      source_validation:
        split_key: val_idxs
        batch_size: 8
        num_workers: 1
        sampler:
          _target_: torch.utils.data.sampler.SubsetRandomSampler
      target_validation:
        split_key: val_idxs
        batch_size: 8
        num_workers: 1
        sampler:
          _target_: torch.utils.data.sampler.SubsetRandomSampler
    optimizers:
      label_predictor:
        _target_: torch.optim.Adam
        lr: 0.05
        weight_decay: 0
      domain_classifier:
        _target_: torch.optim.Adam
        lr: 0.02
        weight_decay: 0
    scheduler:
      _target_: torch.optim.lr_scheduler.ExponentialLR
      gamma: 0.8
    # grl_schedule:
    #   _target_: watchmal.model.dann.GradientReversalScheduler
    #   init_lambda: 0
    #   max_lambda: 1.0
    #   gamma: 10
    #   max_iter: 10000

  evaluate:
    data_loaders:
      target_test:
        split_key: test_idxs
        batch_size: 2000
        num_workers: 6
        sampler:
          _target_: torch.utils.data.sampler.SubsetRandomSampler

gpu_list:
- 5
seed: null
dump_path: ./outputs/